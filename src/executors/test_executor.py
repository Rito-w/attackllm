#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•æ‰§è¡Œå™¨
è´Ÿè´£æ‰§è¡Œæ”»å‡»æµ‹è¯•å¹¶å¤„ç†å“åº”ç»“æžœ
"""

import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from models.core import PromptVariation, TestResult, ResponseStatus, save_to_json
from api.client import SiliconFlowClient
from config.config_manager import ConfigManager


class TestExecutor:
    """æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, api_client: Optional[SiliconFlowClient] = None, 
                 config_manager: Optional[ConfigManager] = None):
        if api_client is None:
            from api.client import api_client as default_client
            api_client = default_client
        
        if config_manager is None:
            from config.config_manager import config_manager as default_config
            config_manager = default_config
        
        self.api_client = api_client
        self.config = config_manager.get_api_config()
        
        # æµ‹è¯•ç»Ÿè®¡
        self.total_tests = 0
        self.successful_responses = 0
        self.rejected_requests = 0
        self.error_requests = 0
        self.test_results: List[TestResult] = []
        
        print("ðŸŽ¯ æµ‹è¯•æ‰§è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def execute_single_test(self, prompt_variation: PromptVariation, 
                          max_tokens: int = 1000, temperature: float = 0.7) -> TestResult:
        """
        æ‰§è¡Œå•ä¸ªæµ‹è¯•
        
        Args:
            prompt_variation: æ”»å‡»æç¤ºè¯å˜ä½“
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            æµ‹è¯•ç»“æžœ
        """
        print(f"ðŸ” æ‰§è¡Œæµ‹è¯•: {prompt_variation.method_name}")
        
        # å‘é€APIè¯·æ±‚
        status, response_text, metadata = self.api_client.send_request(
            prompt=prompt_variation.generated_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        # åˆ›å»ºæµ‹è¯•ç»“æžœ
        test_result = TestResult(
            prompt_id=prompt_variation.id,
            method_name=prompt_variation.method_name,
            request_prompt=prompt_variation.generated_prompt,
            response_text=response_text,
            response_status=status,
            api_key_used=f"key_{metadata.get('api_key_index', 0)}",
            response_time=metadata.get('response_time', 0.0),
            http_status_code=metadata.get('http_status_code')
        )
        
        # å¦‚æžœæœ‰é”™è¯¯ä¿¡æ¯ï¼Œè®°å½•åˆ°ç»“æžœä¸­
        if 'error' in metadata:
            test_result.error_message = metadata['error']
        
        # æ›´æ–°ç»Ÿè®¡
        self.total_tests += 1
        if status == ResponseStatus.SUCCESS:
            self.successful_responses += 1
        elif status == ResponseStatus.REJECTED:
            self.rejected_requests += 1
        else:
            self.error_requests += 1
        
        self.test_results.append(test_result)
        
        return test_result
    
    def execute_batch_test(self, prompt_variations: List[PromptVariation], 
                          max_workers: int = 1, max_tokens: int = 1000, 
                          temperature: float = 0.7, 
                          progress_callback: Optional[callable] = None) -> List[TestResult]:
        """
        æ‰§è¡Œæ‰¹é‡æµ‹è¯•
        
        Args:
            prompt_variations: æ”»å‡»æç¤ºè¯å˜ä½“åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆå»ºè®®ä¸º1ä»¥éµå®ˆé€ŸçŽ‡é™åˆ¶ï¼‰
            max_tokens: æœ€å¤§tokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            progress_callback: è¿›åº¦å›žè°ƒå‡½æ•°
        
        Returns:
            æµ‹è¯•ç»“æžœåˆ—è¡¨
        """
        print(f"ðŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•...")
        print(f"   æµ‹è¯•æ•°é‡: {len(prompt_variations)}")
        print(f"   å¹¶å‘æ•°: {max_workers}")
        
        results = []
        start_time = time.time()
        
        if max_workers == 1:
            # å•çº¿ç¨‹æ‰§è¡Œï¼ˆæŽ¨èï¼Œé¿å…é€ŸçŽ‡é™åˆ¶é—®é¢˜ï¼‰
            for i, prompt_variation in enumerate(prompt_variations):
                try:
                    result = self.execute_single_test(prompt_variation, max_tokens, temperature)
                    results.append(result)
                    
                    # è¿›åº¦å›žè°ƒ
                    if progress_callback:
                        progress_callback(i + 1, len(prompt_variations), result)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if (i + 1) % 10 == 0 or (i + 1) == len(prompt_variations):
                        elapsed = time.time() - start_time
                        avg_time = elapsed / (i + 1)
                        remaining = (len(prompt_variations) - i - 1) * avg_time
                        print(f"   è¿›åº¦: {i + 1}/{len(prompt_variations)} "
                              f"({(i + 1)/len(prompt_variations)*100:.1f}%) "
                              f"é¢„è®¡å‰©ä½™: {remaining:.1f}ç§’")
                
                except Exception as e:
                    print(f"âŒ æµ‹è¯•å¤±è´¥ [{prompt_variation.method_name}]: {e}")
                    # åˆ›å»ºé”™è¯¯ç»“æžœ
                    error_result = TestResult(
                        prompt_id=prompt_variation.id,
                        method_name=prompt_variation.method_name,
                        request_prompt=prompt_variation.generated_prompt,
                        response_text=f"æµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}",
                        response_status=ResponseStatus.ERROR,
                        api_key_used="unknown",
                        error_message=str(e)
                    )
                    results.append(error_result)
                    self.test_results.append(error_result)
                    self.total_tests += 1
                    self.error_requests += 1
        
        else:
            # å¤šçº¿ç¨‹æ‰§è¡Œï¼ˆè°¨æ…Žä½¿ç”¨ï¼‰
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_prompt = {
                    executor.submit(self.execute_single_test, prompt, max_tokens, temperature): prompt
                    for prompt in prompt_variations
                }
                
                # æ”¶é›†ç»“æžœ
                completed = 0
                for future in as_completed(future_to_prompt):
                    prompt_variation = future_to_prompt[future]
                    try:
                        result = future.result()
                        results.append(result)
                        completed += 1
                        
                        # è¿›åº¦å›žè°ƒ
                        if progress_callback:
                            progress_callback(completed, len(prompt_variations), result)
                        
                        # æ˜¾ç¤ºè¿›åº¦
                        if completed % 10 == 0 or completed == len(prompt_variations):
                            print(f"   è¿›åº¦: {completed}/{len(prompt_variations)} "
                                  f"({completed/len(prompt_variations)*100:.1f}%)")
                    
                    except Exception as e:
                        print(f"âŒ æµ‹è¯•å¤±è´¥ [{prompt_variation.method_name}]: {e}")
                        completed += 1
        
        elapsed_time = time.time() - start_time
        print(f"âœ… æ‰¹é‡æµ‹è¯•å®Œæˆ")
        print(f"   æ€»è€—æ—¶: {elapsed_time:.1f}ç§’")
        print(f"   å¹³å‡è€—æ—¶: {elapsed_time/len(prompt_variations):.2f}ç§’/æµ‹è¯•")
        print(f"   æˆåŠŸå“åº”: {self.successful_responses}")
        print(f"   è¢«æ‹’ç»: {self.rejected_requests}")
        print(f"   é”™è¯¯: {self.error_requests}")
        
        return results
    
    def save_test_results(self, results: List[TestResult], 
                         file_path: Optional[str] = None) -> str:
        """
        ä¿å­˜æµ‹è¯•ç»“æžœåˆ°æ–‡ä»¶
        
        Args:
            results: æµ‹è¯•ç»“æžœåˆ—è¡¨
            file_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        
        Returns:
            å®žé™…ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = f"data/results/test_results_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        results_data = [result.to_dict() for result in results]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        save_to_json(results_data, file_path)
        
        print(f"âœ… æµ‹è¯•ç»“æžœå·²ä¿å­˜åˆ°: {file_path}")
        return file_path
    
    def export_successful_responses_to_markdown(self, results: List[TestResult], 
                                              file_path: Optional[str] = None) -> str:
        """
        å°†æˆåŠŸå“åº”å¯¼å‡ºåˆ°Markdownæ–‡ä»¶ä¾›äººå·¥å®¡æ ¸
        
        Args:
            results: æµ‹è¯•ç»“æžœåˆ—è¡¨
            file_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æžœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        
        Returns:
            å®žé™…ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if file_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            file_path = f"data/results/successful_responses_{timestamp}.md"
        
        # ç­›é€‰æˆåŠŸå“åº”
        successful_results = [r for r in results if r.is_successful_response()]
        
        if not successful_results:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸå“åº”éœ€è¦å¯¼å‡º")
            return ""
        
        # ç”ŸæˆMarkdownå†…å®¹
        markdown_content = self._generate_markdown_report(successful_results)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"âœ… æˆåŠŸå“åº”å·²å¯¼å‡ºåˆ°Markdown: {file_path}")
        print(f"   æˆåŠŸå“åº”æ•°é‡: {len(successful_results)}")
        
        return file_path
    
    def _generate_markdown_report(self, successful_results: List[TestResult]) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            successful_results: æˆåŠŸçš„æµ‹è¯•ç»“æžœåˆ—è¡¨
        
        Returns:
            Markdownæ ¼å¼çš„æŠ¥å‘Šå†…å®¹
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        markdown_lines = [
            f"# å¤§æ¨¡åž‹å®‰å…¨æ”»å‡»æµ‹è¯•ç»“æžœæŠ¥å‘Š",
            f"",
            f"**ç”Ÿæˆæ—¶é—´**: {timestamp}",
            f"**æˆåŠŸå“åº”æ•°é‡**: {len(successful_results)}",
            f"",
            f"---",
            f""
        ]
        
        # æŒ‰æ–¹æ³•åˆ†ç»„
        method_groups = {}
        for result in successful_results:
            method_name = result.method_name
            if method_name not in method_groups:
                method_groups[method_name] = []
            method_groups[method_name].append(result)
        
        # ä¸ºæ¯ä¸ªæ–¹æ³•ç”ŸæˆæŠ¥å‘Š
        for method_name, method_results in method_groups.items():
            markdown_lines.extend([
                f"## {method_name}",
                f"",
                f"**æˆåŠŸæ¬¡æ•°**: {len(method_results)}",
                f""
            ])
            
            for i, result in enumerate(method_results, 1):
                markdown_lines.extend([
                    f"### æµ‹è¯•æ¡ˆä¾‹ {i}",
                    f"",
                    f"**æµ‹è¯•æ—¶é—´**: {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
                    f"**å“åº”æ—¶é—´**: {result.response_time:.2f}ç§’",
                    f"**HTTPçŠ¶æ€ç **: {result.http_status_code}",
                    f"",
                    f"**æ”»å‡»æç¤ºè¯**:",
                    f"```",
                    result.request_prompt,
                    f"```",
                    f"",
                    f"**æ¨¡åž‹å›žç­”**:",
                    f"```",
                    result.response_text,
                    f"```",
                    f"",
                    f"**å‡½æ•°æ¨¡æ¿**: `{method_name}(harmful_prompt)`",
                    f"",
                    f"---",
                    f""
                ])
        
        return "\n".join(markdown_lines)
    
    def get_test_statistics(self) -> Dict[str, Any]:
        """
        èŽ·å–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if self.total_tests == 0:
            return {
                "total_tests": 0,
                "successful_responses": 0,
                "rejected_requests": 0,
                "error_requests": 0,
                "success_rate": 0.0,
                "rejection_rate": 0.0,
                "error_rate": 0.0,
                "method_statistics": {},
                "avg_response_time": 0.0
            }
        
        # æŒ‰æ–¹æ³•ç»Ÿè®¡
        method_stats = {}
        total_response_time = 0
        
        for result in self.test_results:
            method_name = result.method_name
            if method_name not in method_stats:
                method_stats[method_name] = {
                    "total": 0,
                    "successful": 0,
                    "rejected": 0,
                    "errors": 0,
                    "avg_response_time": 0.0
                }
            
            method_stats[method_name]["total"] += 1
            total_response_time += result.response_time
            
            if result.response_status == ResponseStatus.SUCCESS:
                method_stats[method_name]["successful"] += 1
            elif result.response_status == ResponseStatus.REJECTED:
                method_stats[method_name]["rejected"] += 1
            else:
                method_stats[method_name]["errors"] += 1
        
        # è®¡ç®—æ¯ä¸ªæ–¹æ³•çš„æˆåŠŸçŽ‡å’Œå¹³å‡å“åº”æ—¶é—´
        for method_name, stats in method_stats.items():
            if stats["total"] > 0:
                stats["success_rate"] = stats["successful"] / stats["total"]
                method_results = [r for r in self.test_results if r.method_name == method_name]
                stats["avg_response_time"] = sum(r.response_time for r in method_results) / len(method_results)
        
        return {
            "total_tests": self.total_tests,
            "successful_responses": self.successful_responses,
            "rejected_requests": self.rejected_requests,
            "error_requests": self.error_requests,
            "success_rate": self.successful_responses / self.total_tests,
            "rejection_rate": self.rejected_requests / self.total_tests,
            "error_rate": self.error_requests / self.total_tests,
            "method_statistics": method_stats,
            "avg_response_time": total_response_time / self.total_tests if self.total_tests > 0 else 0.0
        }
    
    def reset_statistics(self) -> None:
        """é‡ç½®æµ‹è¯•ç»Ÿè®¡"""
        self.total_tests = 0
        self.successful_responses = 0
        self.rejected_requests = 0
        self.error_requests = 0
        self.test_results.clear()
        print("ðŸ“Š æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    def filter_results_by_status(self, status: ResponseStatus) -> List[TestResult]:
        """
        æŒ‰å“åº”çŠ¶æ€ç­›é€‰ç»“æžœ
        
        Args:
            status: å“åº”çŠ¶æ€
        
        Returns:
            ç­›é€‰åŽçš„ç»“æžœåˆ—è¡¨
        """
        return [result for result in self.test_results if result.response_status == status]
    
    def get_top_performing_methods(self, min_tests: int = 3, top_n: int = 10) -> List[Dict[str, Any]]:
        """
        èŽ·å–è¡¨çŽ°æœ€å¥½çš„æ”»å‡»æ–¹æ³•
        
        Args:
            min_tests: æœ€å°‘æµ‹è¯•æ¬¡æ•°
            top_n: è¿”å›žå‰Nä¸ªæ–¹æ³•
        
        Returns:
            æŽ’åºåŽçš„æ–¹æ³•æ€§èƒ½åˆ—è¡¨
        """
        method_performance = {}
        
        for result in self.test_results:
            method_name = result.method_name
            if method_name not in method_performance:
                method_performance[method_name] = {
                    "method_name": method_name,
                    "total_tests": 0,
                    "successful_responses": 0,
                    "success_rate": 0.0,
                    "avg_response_time": 0.0,
                    "response_times": []
                }
            
            perf = method_performance[method_name]
            perf["total_tests"] += 1
            perf["response_times"].append(result.response_time)
            
            if result.response_status == ResponseStatus.SUCCESS:
                perf["successful_responses"] += 1
        
        # è®¡ç®—æˆåŠŸçŽ‡å’Œå¹³å‡å“åº”æ—¶é—´
        for method_name, perf in method_performance.items():
            if perf["total_tests"] >= min_tests:
                perf["success_rate"] = perf["successful_responses"] / perf["total_tests"]
                perf["avg_response_time"] = sum(perf["response_times"]) / len(perf["response_times"])
            del perf["response_times"]  # åˆ é™¤ä¸´æ—¶æ•°æ®
        
        # ç­›é€‰å¹¶æŽ’åº
        qualified_methods = [
            perf for perf in method_performance.values() 
            if perf["total_tests"] >= min_tests
        ]
        
        qualified_methods.sort(key=lambda x: x["success_rate"], reverse=True)
        
        return qualified_methods[:top_n]


# å…¨å±€æµ‹è¯•æ‰§è¡Œå™¨å®žä¾‹
test_executor = TestExecutor()